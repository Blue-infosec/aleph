{"name":"Aleph","tagline":"OpenSource Malware Analysis Pipeline System","body":"## What?\r\nAleph is designed to pipeline the analysis of malware samples. It has a series of collectors that will gather samples from many sources and shove them into the pipeline. The sample manager has a series of plugins that are ran against the sample and returns found data into JSON form.\r\n\r\nThese JSON data can be further processed and queried in a objective manner instead of *grepping and regexing*.\r\n\r\n## How to Work?\r\nThe main Aleph daemon is a loose-coupled python application and library. These are composed by the Aleph Service that spawns:\r\n\r\n1. The Collectors. These are responsible for going to multiple sources (Filesystem folder, IMAP folder, FTP directory etc) and collect all the files there, store locally and add them to the processing queue. Each collector runs in its own process (fork).\r\n2. Multiple (quantity is configurable) parallel SampleManager services (that will pull samples from the work queue and process them) and run the plugins that receives the sample path and return the JSON object of found artifacts.\r\n3. The sample object is converted to JSON along with its data and is stored into an Elasticsearch backend.\r\n\r\n## Architect Aleph\r\n\r\n![Architect Aleph System](https://app.box.com/s/aa33xkcbdp6c37akk9pel0pvfbb3r74o)\r\n\r\n## Installing Aleph\r\n### Requirements\r\nIn order to get a clean and nice install, you should download some requirements:\r\nUbuntu/Debian\r\n\r\n\tapt-get install python-pyrex libffi-dev libfuzzy-dev python-dateutil libsqlite3-dev\r\n\t\r\n\r\n#### ElasticSearch\r\nFirst if you don't have an [Elasticsearch](www.elasticsearch.org) instance ready, you must install one. \r\n\r\nFor Debian/Ubuntu/Redhat/Fedora/CentOS (yum + apt basically) users, follow [this guide](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-repositories.html).\r\n\r\n** Remember: Elasticsearh uses JVM, so you also must install it =) **\r\n\r\n#### Python modules\r\nWe strongly suggest that you use python's virtual environment so you don't pollute the rest of your OS installation with python modules. To make a contained virtual environment, install _virtualenv_ with _pip_:\r\n\r\n    pip install virtualenv\r\n\r\nGo to the desired Aleph installation folder and type the following to create and activate your virtual environment:\r\n\r\n    virtualenv venv # 'venv' can be any name\r\n    source venv/bin/activate\r\n\r\nThere will be the environment name (venv) appended to your PS1 variable:\r\n\r\n    (venv)(2014-08-19 17:36:%)(~/opt/aleph/)\r\n\r\nAll python modules required are listed on the _requirements.txt_ file on the root repository folder. You can install all of them at once using _pip_:\r\n\r\n    pip install -r requirements.txt\r\n\r\nThen clone the repository and copy the settings file:\r\n\r\n    git clone https://github.com/merces/aleph.git --branch aleph-python --single-branch .\r\n    cp aleph/settings.py.orig aleph/settings.py\r\n\r\nEdit settings.py and add a local source (a folder where Aleph will search for samples - **WARNING: ALEPH WILL MOVE THE SAMPLE THUS REMOVING FROM THE ORIGINAL FOLDER**) _The folder must exists as Aleph won't try to create them_\r\n\r\n    SAMPLE_SOURCES = [\r\n        ('local', {'path': '/opt/aleph/unprocessed_samples'}),\r\n    ]\r\n\r\nReview your Elasticsearch installation URI\r\n\r\n    ELASTICSEARCH_URI = '127.0.0.1:9200'\r\n\r\n** Workaround step **\r\nAs I still finish some of the code, there are some folders that are not on the repository and must be created manually and set accordingly on the *settings.py* file:\r\n\r\n    SAMPLE_TEMP_DIR = '/opt/aleph/temp'\r\n    SAMPLE_STORAGE_DIR = '/opt/aleph/samples'\r\n\r\nRemember to verify folders permissioning.\r\nAnd Aleph is ready to run!\r\n\r\n#### Running \r\nGo to Aleph folder, activate the virtual environment and run the bin/aleph-server.py as following:\r\n\r\n    cd /opt/aleph/\r\n    source venv/bin/activate\r\n    ./bin/aleph-server.py\r\n\r\nAnd that's it. Check your logs under log/aleph.log to any troubleshooting.\r\n\r\n#### Install the Web interface(Webui)\r\nEdit the \"SERVER_NAME\" constant at your settings.py file.\r\n\tex: SERVER_NAME = 'mydomain.com:90'\r\n\t\r\nthen create the following entry after \"SAMPLE_SUBMIT_FOLDER\":\r\n\r\n\tSECRET_KEY = 'Pu7s0m3cryp7l337here' #do not use this ;)\r\n\r\nChange \"SAMPLE_SUBMIT_FOLDER\" to:\r\n\r\n\tSAMPLE_SUBMIT_FOLDER= '/some/path' #where samples will be submitted from webui\r\n\r\n**Note: After this changes comment that two entries (SECRET_KEY and SAMPLE_SUBMIT_FOLDER)**\r\n\r\nSetup your database:\r\n\r\n\tpython bin/db_create.py\r\n\r\nRun the\twebui script:\r\n\t\r\n\tbin/aleph-webui.sh\r\n\t\r\nTo access your web interface open your favorite browser at http://SERVER_NAME #That value you changed before.\r\n\t\r\n\tLogin: admin\r\n\tPassword: changeme12!\r\n\t\r\n\t\r\n**Note: For sake of Security's God, CHANGE YOUR PASSWORD! ;)**\r\n\t\r\nBut if you do not like our webinterface you still can use other softwares  to review and query data on elasticsearch. I strongly suggest this [Chrome REST client plugin](https://chrome.google.com/webstore/detail/postman-rest-client/fdmmgilgnpjigdojojpjoooidkmcomcm?hl=en) or the great [Kibana](http://www.elasticsearch.org/guide/en/kibana/current/working-with-queries-and-filters.html)\r\n\r\n##Currently implemented\r\n### Collectors\r\n* **FileCollector:** grabs samples from a local directory\r\n* **MailCollector:** grabs samples from email attachments on a IMAP folder\r\n\r\n### Plugins\r\n* **PEInfo:** extracts info from PE files such as entrypoint, number of sections and some PE characteristics(SEH/ASLR/DEP).\r\n* **TrID:** check the filetype of a sample.\r\n* **ZipArchive:** extracts zip files and puts their contents back into analysis queue.\r\n* **TarZipArchive:** extracts tar files and puts their contents back into analysis queue\r\n* **RARArchive:** extracts zip files and puts their contents back into analysis queue\r\n* **VirusTotal:** check a sample SHA256 hash against VirusTotal database and get the report. If that hash doesn't exist, send the file to analise.\r\n* **Email:** extracts information about headers, email source, email destination and subject.\r\n* **URLExtractor:**\r\n* **URIParser:** \r\n* **Strings:** extracts strings from sample into three categories: All Strings, URI Strings and Filename Strings (not 100% but we do our best).","google":"UA-63850202-1","note":"Don't delete this file! It's used internally to help with page regeneration."}